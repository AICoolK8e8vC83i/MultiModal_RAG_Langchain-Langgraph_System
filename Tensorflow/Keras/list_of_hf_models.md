# Cutting-Edge Hugging Face Models

## Text Generation

1. **mistralai/Mistral-Small-3.1-24B-Instruct-2503**  
   An instruction-tuned model with 24 billion parameters, designed for versatile text generation tasks.  
   :contentReference[oaicite:0]{index=0}&#8203;:contentReference[oaicite:1]{index=1}

2. **ds4sd/SmolDocling-256M-preview**  
   A compact 256 million-parameter model optimized for document understanding and linguistic tasks.  
   :contentReference[oaicite:2]{index=2}&#8203;:contentReference[oaicite:3]{index=3}

3. **manycore-research/SpatialLM-Llama-1B**  
   A 1 billion-parameter model focusing on spatial language modeling, enhancing understanding of spatial relationships in text.  
   :contentReference[oaicite:4]{index=4}&#8203;:contentReference[oaicite:5]{index=5}

4. **google/gemma-3-27b-it**  
   A 27 billion-parameter model tailored for Italian language processing, offering state-of-the-art performance in Italian NLP tasks.  
   :contentReference[oaicite:6]{index=6}&#8203;:contentReference[oaicite:7]{index=7}

5. **Qwen/QwQ-32B**  
   A 32 billion-parameter model excelling in various text generation tasks, known for its fluency and coherence.  
   :contentReference[oaicite:8]{index=8}&#8203;:contentReference[oaicite:9]{index=9}

6. **LGAI-EXAONE/EXAONE-Deep-32B**  
   A deep learning model with 32 billion parameters, designed for complex text generation and understanding tasks.  
   :contentReference[oaicite:10]{index=10}&#8203;:contentReference[oaicite:11]{index=11}

7. **deepseek-ai/DeepSeek-R1**  
   A robust model for deep text analysis and generation, suitable for research and industrial applications.  
   :contentReference[oaicite:12]{index=12}&#8203;:contentReference[oaicite:13]{index=13}

8. **nvidia/Llama-3_3-Nemotron-Super-49B-v1**  
   A 49 billion-parameter model developed by NVIDIA, offering superior performance in large-scale text generation tasks.  
   :contentReference[oaicite:14]{index=14}&#8203;:contentReference[oaicite:15]{index=15}

9. **starvector/starvector-8b-im2svg**  
   An 8 billion-parameter model specialized in converting images to SVG format, bridging the gap between visual and textual data.  
   :contentReference[oaicite:16]{index=16}&#8203;:contentReference[oaicite:17]{index=17}

10. **CohereForAI/c4ai-command-a-03-2025**  
    A command-oriented AI model designed for executing and understanding complex instructions in natural language.  
    :contentReference[oaicite:18]{index=18}

## Text-to-Speech

11. **sesame/csm-1b**  
    A 1 billion-parameter model for high-quality text-to-speech applications, delivering natural and expressive speech synthesis.  
    :contentReference[oaicite:19]{index=19}

12. **canopylabs/orpheus-3b-0.1-ft**  
    A 3 billion-parameter fine-tuned model for text-to-speech tasks, known for its clarity and naturalness in speech output.  
    :contentReference[oaicite:20]{index=20}

13. **SparkAudio/Spark-TTS-0.5B**  
    A 500 million-parameter model offering efficient and high-quality text-to-speech conversion, suitable for real-time applications.  
    :contentReference[oaicite:21]{index=21}

14. **hexgrad/Kokoro-82M**  
    An 82 million-parameter model focusing on lightweight and efficient text-to-speech synthesis, ideal for mobile applications.  
    :contentReference[oaicite:22]{index=22}

## Image-to-Text

15. **tencent/Hunyuan3D-2mv**  
    A model capable of generating 3D representations from images, enhancing visual understanding and applications in 3D modeling.  
    :contentReference[oaicite:23]{index=23}

16. **google/gemma-3-4b-it**  
    A 4 billion-parameter model designed for Italian image-to-text tasks, improving the accuracy of image descriptions in Italian.  
    :contentReference[oaicite:24]{index=24}

17. **mlabonne/gemma-3-27b-it-abliterated**  
    An advanced version of the gemma-3 series, offering enhanced capabilities in image-to-text generation for the Italian language.  
    :contentReference[oaicite:25]{index=25}

## Text-to-Image

18. **black-forest-labs/FLUX.1-dev**  
    A developmental model focusing on generating high-quality images from textual descriptions, pushing the boundaries of text-to-image synthesis.  
    :contentReference[oaicite:26]{index=26}

19. **ByteDance/InfiniteYou**  
    A model developed by ByteDance, specializing in generating infinite variations of images from textual prompts, showcasing creativity in AI.  
    :contentReference[oaicite:27]{index=27}

20. **Skywork/Skywork-R1V-38B**  
    A 38 billion-parameter model designed for generating realistic images from text, enhancing applications in content creation and design.  
    :contentReference[oaicite:28]{index=28}

## Image-to-Video

21. **stabilityai/stable-virtual-camera**  
    A model that generates virtual camera views from input images, enabling dynamic perspectives in visual content.  
    :contentReference[oaicite:29]{index=29}

## Robotics

22. **nvidia/GR00T-N1-2B**  
    A 2 billion-parameter model by NVIDIA, focusing on robotics applications, enhancing robot perception and decision-making.  
    :contentReference[oaicite:30]{index=30}

## Automatic Speech Recognition

23. **nvidia/canary-1b-flash**  
   
::contentReference[oaicite:31]{index=31}
 
# Cutting-Edge Hugging Face Models (Continued)

## Text Generation

24. **meta-llama/Llama-3.2-3B-Instruct**  
    A 3 billion-parameter instruction-tuned model from Meta, optimized for following complex instructions in text generation tasks.  
    ([huggingface.co](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct))

25. **allenai/OLMo-2-0325-32B-Instruct**  
    A 32 billion-parameter model from AllenAI, designed for instruction-based text generation with enhanced reasoning capabilities.  
    ([huggingface.co](https://huggingface.co/allenai/OLMo-2-0325-32B-Instruct))

26. **meta-llama/Llama-3.2-1B**  
    A 1 billion-parameter model from Meta, offering efficient text generation suitable for resource-constrained environments.  
    ([huggingface.co](https://huggingface.co/meta-llama/Llama-3.2-1B))

27. **open-r1/OlympicCoder-7B**  
    A 7 billion-parameter model tailored for code generation and understanding, facilitating software development tasks.  
    ([huggingface.co](https://huggingface.co/open-r1/OlympicCoder-7B))

28. **open-r1/OlympicCoder-32B**  
    A 32 billion-parameter version of OlympicCoder, providing advanced capabilities in code synthesis and analysis.  
    ([huggingface.co](https://huggingface.co/open-r1/OlympicCoder-32B))

29. **perplexity-ai/r1-1776**  
    A model focusing on generating human-like text with high perplexity, enhancing creativity in text generation.  
    ([huggingface.co](https://huggingface.co/perplexity-ai/r1-1776))

30. **mistralai/Mistral-7B-Instruct-v0.3**  
    A 7 billion-parameter instruction-tuned model from MistralAI, optimized for following user instructions in text generation.  
    ([huggingface.co](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3))

31. **meta-llama/Llama-3.3-70B-Instruct**  
    A 70 billion-parameter instruction-tuned model from Meta, offering state-of-the-art performance in complex text generation tasks.  
    ([huggingface.co](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct))

32. **deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B**  
    A distilled 1.5 billion-parameter model from DeepSeek AI, providing efficient text generation with reduced computational requirements.  
    ([huggingface.co](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B))

33. **NousResearch/DeepHermes-3-Mistral-24B-Preview**  
    A 24 billion-parameter model preview from NousResearch, focusing on high-quality text generation with Mistral architecture.  
    ([huggingface.co](https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview))

34. **lmstudio-community/Mistral-Small-3.1-24B-Instruct-2503-GGUF**  
    A 24 billion-parameter instruction-tuned model from LMStudio Community, optimized for versatile text generation tasks.  
    ([huggingface.co](https://huggingface.co/lmstudio-community/Mistral-Small-3.1-24B-Instruct-2503-GGUF))

35. **microsoft/phi-4**  
    A model from Microsoft focusing on advanced text generation with enhanced coherence and context understanding.  
    ([huggingface.co](https://huggingface.co/microsoft/phi-4))

36. **Qwen/Qwen2.5-7B-Instruct**  
    A 7 billion-parameter instruction-tuned model from Qwen, designed for effective instruction following in text generation.  
    ([huggingface.co](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct))

## Code Generation

37. **stabilityai/stable-code-3b**  
    A 3 billion-parameter model from Stability AI, optimized for code generation tasks across multiple programming languages.  
    ([huggingface.co](https://huggingface.co/stabilityai/stable-code-3b))

38. **stabilityai/stable-code-instruct-3b**  
    An instruction-tuned variant of stable-code-3b, designed to follow natural language instructions for code generation.  
    ([huggingface.co](https://huggingface.co/stabilityai/stable-code-instruct-3b))

39. **cerebras/btlm-3b-8k-base**  
    A 3 billion-parameter model from Cerebras, offering long-context code generation capabilities with an 8K context window.  
    ([huggingface.co](https://huggingface.co/cerebras/btlm-3b-8k-base))

## Multilingual Models

40. **google/mt5-xxl**  
    A multilingual text-to-text transfer transformer model from Google, supporting over 100 languages for various NLP tasks.  
    ([huggingface.co](https://huggingface.co/google/mt5-xxl))

41. **facebook/mbart-large-50**  
    A multilingual sequence-to-sequence model from Facebook, supporting translation and text generation in 50 languages.  
    ([huggingface.co](https://huggingface.co/facebook/mbart-large-50))

42. **Helsinki-NLP/opus-mt-en-zh**  
    A model from Helsinki-NLP specialized in English to Chinese translation, providing high-quality machine translation.  
    ([huggingface.co](https://huggingface.co/Helsinki-NLP/opus-mt-en-zh))

## Vision-Language Models

43. **openai/clip-vit-large-patch14**  
    A vision-language model from OpenAI that connects images and text, useful for tasks like image captioning and visual search.  
    ([huggingface.co](https://huggingface.co/openai/clip-vit-large-patch14))

44. **Salesforce/blip-image-captioning-base**  
    A model from Salesforce designed for generating captions for images, enhancing image understanding and accessibility.  
    ([huggingface.co](https://huggingface.co/Salesforce/blip-image-captioning-base))

45. **microsoft/git-large-coco**  
    A vision-language model from Microsoft trained on the COCO dataset, suitable for image captioning and visual question answering.  
    ([huggingface.co](https://huggingface.co/microsoft/git
::contentReference[oaicite:0]{index=0}
 
Perfect — I’ll put together a diverse list of 55 cutting-edge Hugging Face models spanning text generation, multimodal (vision-language), code generation, speech, and more. I’ll include a mix of newly released and top-performing models across use cases so your list stays bleeding edge.

Give me a moment to curate that for you.

## Text Generation
46. **mistralai/Mistral-7B-Instruct-v0.3**  
A 7B-parameter instruction-tuned transformer model from Mistral AI, designed for general text generation with strong performance for its size ([robotics-diffusion-transformer/rdt-1b · Hugging Face](https://huggingface.co/robotics-diffusion-transformer/rdt-1b#:~:text=RDT)).  
([Hugging Face link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3))

47. **meta-llama/Llama-2-70b-chat-hf**  
Meta’s 70B-parameter LLaMA 2 Chat model, an optimized dialogue AI that excels at following instructions and producing coherent answers ([facebook/seamless-m4t-large · Hugging Face](https://huggingface.co/facebook/seamless-m4t-large#:~:text=SeamlessM4T%20is%20a%20collection%20of,effortlessly%20through%20speech%20and%20text)).  
([Hugging Face link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf))

48. **tiiuae/falcon-180B-chat**  
A 180B-parameter causal decoder model by TII with state-of-the-art open-model performance ([Spread Your Wings: Falcon 180B is here - Hugging Face](https://huggingface.co/blog/falcon-180b#:~:text=Falcon%20180B%20sets%20a%20new,model%2C%20with%20180%20billion%20parameters)), tuned for conversational and instructional responses.  
([Hugging Face link](https://huggingface.co/tiiuae/falcon-180B-chat))

49. **deepseek-ai/DeepSeek-V3-Base**  
A massive MoE (Mixture-of-Experts) language model totaling 671B parameters (37B active per token) ([deepseek-ai/DeepSeek-V3-Base · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base#:~:text=1)), pushing the frontier of open-source reasoning and knowledge tasks.  
([Hugging Face link](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base))

50. **CohereForAI/c4ai-command-a-03-2025**  
Cohere’s 111B-parameter Command model, optimized for high-quality, fast interactive AI with an extended context window of up to 256K tokens ([CohereForAI/c4ai-command-a-03-2025 · Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-a-03-2025#:~:text=C4AI%20Command%20A%20is%20an,deployable%20on%20just%20two%20GPUs)) ([CohereForAI/c4ai-command-a-03-2025 · Hugging Face](https://huggingface.co/CohereForAI/c4ai-command-a-03-2025#:~:text=%2A%20Model%3A%20c4ai,Context%20length%3A%20256K)).  
([Hugging Face link](https://huggingface.co/CohereForAI/c4ai-command-a-03-2025))

51. **google/flan-ul2**  
A 20B unified encoder-decoder model from Google fine-tuned on instructions, capable of step-by-step reasoning and multi-turn dialogue in a text-to-text format ([Add FLAN-UL2 · Issue #21917 · huggingface/transformers - GitHub](https://github.com/huggingface/transformers/issues/21917#:~:text=Add%20FLAN,UL2)).  
([Hugging Face link](https://huggingface.co/google/flan-ul2))

52. **WizardLMTeam/WizardMath-70B-V1.0**  
A 70B LLaMA-derived model specialized for mathematical reasoning, augmented via Evol-Instruct to achieve top-tier math problem-solving performance (pass@1 22.7% on MATH dataset ([WizardLMTeam/WizardLM-70B-V1.0 - Hugging Face](https://huggingface.co/WizardLMTeam/WizardLM-70B-V1.0#:~:text=WizardLMTeam%2FWizardLM,source%20LLM))).  
([Hugging Face link](https://huggingface.co/WizardLMTeam/WizardMath-70B-V1.0))

53. **mosaicml/mpt-30b-chat**  
An open 30B-parameter chat model by MosaicML, with 8K token context, geared towards long-form conversations and stories while running efficiently on commercial hardware.  
([Hugging Face link](https://huggingface.co/mosaicml/mpt-30b-chat))

54. **WizardLMTeam/WizardLM-70B-V1.0**  
A 70B instruction-tuned LLM that uses evolutive data augmentation to greatly improve follow-up instruction following and complex question answering over the base model ([WizardLMTeam/WizardCoder-Python-34B-V1.0 · Hugging Face](https://huggingface.co/WizardLMTeam/WizardCoder-Python-34B-V1.0#:~:text=%5B2024%2F01%2F04%5D%20We%20released%20WizardCoder,Plus)).  
([Hugging Face link](https://huggingface.co/WizardLMTeam/WizardLM-70B-V1.0))

55. **baichuan-inc/Baichuan-13B-Chat**  
A 13B bilingual chat model from Baichuan AI, excelling at Chinese and English dialogue with strong performance comparable to larger models, released under an open license in 2023.  
([Hugging Face link](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat))

56. **stabilityai/stablelm-tuned-alpha-7b**  
Stability AI’s 7B-parameter StableLM (Alpha) fine-tuned for chat, aimed at helpful and safe conversational responses, demonstrating the capabilities of smaller open-source LLMs.  
([Hugging Face link](https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b))

## Vision-Language Models
57. **HuggingFaceM4/idefics-80b-instruct**  
An 80B-parameter open multimodal model (Flamingo architecture) that accepts interleaved images and text and generates text, reproducing DeepMind’s visual language capabilities for describing and reasoning about images ([Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model](https://huggingface.co/blog/idefics#:~:text=IDEFICS%20is%20an%2080%20billion,grounded%20in%20multiple%20images%2C%20etc)).  
([Hugging Face link](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct))

58. **liuhaotian/llava-llama-2-13b-chat-lightning-preview**  
LLaVA (Large Language and Vision Assistant) combining a CLIP visual encoder with a LLaMA 2 13B chat model ([liuhaotian/llava-llama-2-13b-chat-lightning-preview · Hugging Face](https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview#:~:text=Model%20type%3A%20LLaVA%20is%20an,based%20on%20the%20transformer%20architecture)), allowing users to have conversational Q&A and descriptions about input images.  
([Hugging Face link](https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview))

59. **openflamingo/OpenFlamingo-9B-vitl-mpt7b**  
An open implementation of DeepMind’s Flamingo that pairs a ViT-L/14 image encoder with a 7B language model ([openflamingo/OpenFlamingo-9B-vitl-mpt7b - Hugging Face](https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b#:~:text=OpenFlamingo%20is%20an%20open%20source,7B%20language)), enabling few-shot visual question answering and multimodal interaction with relatively lightweight infrastructure.  
([Hugging Face link](https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b))

60. **nielsr/imagebind-huge**  
A multi-modal transformer model by Meta that learns a joint embedding space for 6 modalities (text, image, audio, depth, thermal, and motion) ([nielsr/imagebind-huge - Hugging Face](https://huggingface.co/nielsr/imagebind-huge#:~:text=ImageBind%20learns%20a%20joint%20embedding,depth%2C%20thermal%2C%20and%20IMU%20data)), allowing innovative cross-modal retrieval and generation applications.  
([Hugging Face link](https://huggingface.co/nielsr/imagebind-huge))

## Code Generation
61. **codellama/CodeLlama-34b-hf**  
A 34B-parameter code-specialized variant of LLaMA 2 ([Phind/Phind-CodeLlama-34B-v2 - Hugging Face](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2#:~:text=Phind,Dataset%20Details)) by Meta, capable of generating and infilling code in multiple programming languages with improved accuracy and longer context handling (up to 100k tokens).  
([Hugging Face link](https://huggingface.co/codellama/CodeLlama-34b-hf))

62. **WizardLMTeam/WizardCoder-33B-V1.1**  
A 33B code LLM fine-tuned via evolutive instructions that achieves near GPT-4 level on coding benchmarks (≈80% pass@1 on HumanEval) ([WizardLMTeam/WizardCoder-Python-34B-V1.0 · Hugging Face](https://huggingface.co/WizardLMTeam/WizardCoder-Python-34B-V1.0#:~:text=%5B2024%2F01%2F04%5D%20We%20released%20WizardCoder,Plus)) ([WizardLMTeam/WizardCoder-Python-34B-V1.0 · Hugging Face](https://huggingface.co/WizardLMTeam/WizardCoder-Python-34B-V1.0#:~:text=%5B2024%2F01%2F04%5D%20WizardCoder,Plus%20pass%401)), making it one of the most advanced open code generators.  
([Hugging Face link](https://huggingface.co/WizardLMTeam/WizardCoder-33B-V1.1))

63. **Phind/Phind-CodeLlama-34B-v2**  
Phind’s 34B model fine-tuned from Code Llama, specializing in multilingual coding tasks (Python, C/C++, JavaScript, etc.) and reaching high accuracy on coding challenges ([Phind/Phind-CodeLlama-34B-v2 - Hugging Face](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2#:~:text=Phind,Dataset%20Details)).  
([Hugging Face link](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2))

64. **bigcode/StarCoder**  
A 15.5B parameter model trained on 80+ programming languages ([Phind-CodeLlama-34B-v1 - Hugging Face](https://huggingface.co/Phind/Phind-CodeLlama-34B-v1#:~:text=We%27ve%20fine,pass%401%20on%20HumanEval%2C%20respectively)) (including code completion and explanation capabilities), with an extended 8K context and ability to fill in code in the middle of files.  
([Hugging Face link](https://huggingface.co/bigcode/StarCoder))

65. **replit/replit-code-v1-3b**  
A 2.7B parameter model from Replit fine-tuned on a large curated code dataset, tailored for assisting in coding tasks and autocompletion with surprising competency given its small size and specialized training.  
([Hugging Face link](https://huggingface.co/replit/replit-code-v1-3b))

## Multilingual Models
66. **facebook/nllb-200-3.3B**  
No Language Left Behind (3.3B variant) is Meta’s multilingual machine translation model covering 200 languages ([Text translation with facebook/nllb-200-3.3B model - GitHub](https://gist.github.com/glowinthedark/887e715b23473d0054cf5299a8328f8d#:~:text=GitHub%20gist.github.com%20%20facebook%2Fnllb,has%20been%20updated%20recently%20huggingface)), delivering state-of-the-art translation quality even for low-resource languages.  
([Hugging Face link](https://huggingface.co/facebook/nllb-200-3.3B))

67. **facebook/seamless-m4t-v2-large**  
SeamlessM4T is a single unified model for speech and text translation across nearly 100 languages ([facebook/seamless-m4t-v2-large · Hugging Face](https://huggingface.co/facebook/seamless-m4t-v2-large#:~:text=SeamlessM4T%20is%20our%20foundational%20all,text%20in%20nearly%20100%20languages)) ([facebook/seamless-m4t-v2-large · Hugging Face](https://huggingface.co/facebook/seamless-m4t-v2-large#:~:text=,35%20languages%20for%20speech%20output)), enabling speech-to-speech, speech-to-text, and text-to-speech translation in one system.  
([Hugging Face link](https://huggingface.co/facebook/seamless-m4t-v2-large))

68. **THUDM/chatglm2-6b**  
ChatGLM2 is a 6B-parameter bilingual chat model (Chinese and English) from Tsinghua University, which features enhanced training stability, longer context (up to 32K tokens), and more factual responses than its predecessor.  
([Hugging Face link](https://huggingface.co/THUDM/chatglm2-6b))

69. **bigscience/bloomz-7b1**  
A 7.1B parameter multilingual model derived from BLOOM, instruction-tuned to follow prompts in 46 languages and 13 programming languages, capable of cross-lingual dialogue and reasoning.  
([Hugging Face link](https://huggingface.co/bigscience/bloomz-7b1))

70. **google/mt5-xxl**  
The 13B-parameter “mT5 XXL” text-to-text Transformer, a multilingual variant of T5 pre-trained on 101 languages ([mT5 - Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/mt5#:~:text=In%20this%20paper%2C%20we%20introduce,based%20dataset%20covering%20101%20languages)), used for translation, summarization, and question-answering across diverse languages with state-of-the-art results.  
([Hugging Face link](https://huggingface.co/google/mt5-xxl))

## Automatic Speech Recognition (ASR)
71. **openai/whisper-large-v2**  
OpenAI’s 1.5B-parameter Whisper model for speech recognition, capable of transcribing speech in 100 languages with robust accuracy and even performing speech translation to English ([Whisper Speaker Diarization - a Hugging Face Space by Xenova](https://huggingface.co/spaces/Xenova/whisper-speaker-diarization#:~:text=Xenova%20huggingface,languages%20and%20speaker%20segmentation%2C%20respectively)).  
([Hugging Face link](https://huggingface.co/openai/whisper-large-v2))

72. **nvidia/parakeet-rnnt-1.1b**  
A collaboration between NVIDIA NeMo and Suno, this 1.1B RNN-Transducer model provides real-time speech-to-text with high accuracy ([nvidia/parakeet-rnnt-1.1b - Hugging Face](https://huggingface.co/nvidia/parakeet-rnnt-1.1b#:~:text=nvidia%2Fparakeet,ai%20teams)), handling streaming English ASR with low latency suitable for production use.  
([Hugging Face link](https://huggingface.co/nvidia/parakeet-rnnt-1.1b))

73. **facebook/mms-1b-all**  
A 1B-parameter ASR model from Meta’s Massively Multilingual Speech project that can recognize over **1,100+ languages** ([facebook/mms-1b-all - Hugging Face](https://huggingface.co/facebook/mms-1b-all#:~:text=facebook%2Fmms,Example)), dramatically expanding speech recognition to the long tail of low-resource languages.  
([Hugging Face link](https://huggingface.co/facebook/mms-1b-all))

74. **facebook/wav2vec2-large-xlsr-53**  
A 300M-parameter wav2vec 2.0 model pre-trained on 53 languages ([Pretrained Models and Fine-Tuning with HuggingFace - SpeechBrain](https://speechbrain.readthedocs.io/en/latest/tutorials/advanced/pre-trained-models-and-fine-tuning-with-huggingface.html#:~:text=A%20pretrained%20SepFormer%20model%20is,the%20box%20to%20perform)), which can be fine-tuned for highly accurate speech recognition in many languages and was a breakthrough in self-supervised ASR.  
([Hugging Face link](https://huggingface.co/facebook/wav2vec2-large-xlsr-53))

## Text-to-Speech
75. **suno/bark**  
Bark is a transformer-based generative audio model that produces realistic, multilingual speech (and even music or sound effects) from text ([suno/bark - Hugging Face](https://huggingface.co/suno/bark#:~:text=Bark%20is%20a%20transformer,as%20well%20as%20other%20audio)), including the ability to capture voice tone and style for zero-shot voice cloning.  
([Hugging Face link](https://huggingface.co/suno/bark))

76. **facebook/mms-tts-eng**  
Part of Meta’s Massively Multilingual Speech, this text-to-speech model can synthesize English speech (and other languages via related checkpoints) using a single efficient transformer, demonstrating TTS for 1100 languages using limited data ([MMS - Hugging Face](https://huggingface.co/docs/transformers/main/en/model_doc/mms#:~:text=The%20ASR%20model%20checkpoints%20,the%20inference%20documentation%20under%20VITS)).  
([Hugging Face link](https://huggingface.co/facebook/mms-tts-eng))

77. **jbetker/tortoise-tts-v2**  
Tortoise TTS is a high-quality multi-voice text-to-speech system known for its near-human expressiveness and zero-shot voice cloning ([neonbjb/tortoise-tts: A multi-voice TTS system trained with ... - GitHub](https://github.com/neonbjb/tortoise-tts#:~:text=GitHub%20github,Highly%20realistic%20prosody%20and%20intonation)), albeit with a slower autoregressive generation process suitable for offline applications.  
([Hugging Face link](https://huggingface.co/jbetker/tortoise-tts-v2))

78. **microsoft/speecht5_tts**  
A unified Transformer model from Microsoft’s SpeechT5 framework that, after task-specific fine-tuning, can generate natural speech from text as well as do voice conversion ([microsoft/speecht5_tts - Hugging Face](https://huggingface.co/microsoft/speecht5_tts#:~:text=We%20propose%20a%20unified,supervised%20speech%2Ftext%20representation%20learning)), leveraging joint speech-text pre-training.  
([Hugging Face link](https://huggingface.co/microsoft/speecht5_tts))

79. **cshulby/YourTTS**  
YourTTS is a multilingual, multi-speaker TTS built on the VITS architecture that achieves zero-shot voice cloning ([cshulby/YourTTS - Hugging Face](https://huggingface.co/cshulby/YourTTS#:~:text=YourTTS%20is%20a%20model%20for,speaker)) – it can learn a new speaker’s voice from just a sample and produce speech in that voice, even across languages.  
([Hugging Face link](https://huggingface.co/cshulby/YourTTS))

## Text-to-Image
80. **stabilityai/stable-diffusion-xl-base-1.0**  
Stable Diffusion XL is a 2.3B-parameter latent diffusion model for image generation that greatly improves photorealism, fine details, and text rendering in images compared to earlier Stable Diffusion models.  
([Hugging Face link](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0))

81. **DeepFloyd/IF-I-L-v1.0**  
DeepFloyd IF is a cascaded text-to-image system combining a frozen T5-XXL encoder with diffusion models ([DeepFloyd/IF-I-M-v1.0 - Hugging Face](https://huggingface.co/DeepFloyd/IF-I-M-v1.0#:~:text=DeepFloyd,for%20photorealism%20and%20language)): the IF-I-L stage generates a 64×64 image which is then upscaled by super-resolution stages, achieving an FID of 6.66 (state-of-the-art) with excellent text fidelity ([DeepFloyd/IF-I-M-v1.0 · Hugging Face](https://huggingface.co/DeepFloyd/IF-I-M-v1.0#:~:text=DeepFloyd,30K%20score%20of%20%606.66)).  
([Hugging Face link](https://huggingface.co/DeepFloyd/IF-I-L-v1.0))

82. **kandinsky-community/kandinsky-2-2-decoder**  
Kandinsky 2.2 is a diffusion-based text-to-image model (inspired by DALL·E 2) that uses a CLIP ViT-G text prior and an UNet decoder ([Kandinsky 2.2 - Hugging Face](https://huggingface.co/docs/diffusers/en/api/pipelines/kandinsky_v22#:~:text=Kandinsky%202.2%20,G%20and%20the)), capable of generating highly photorealistic images and following multilingual prompts.  
([Hugging Face link](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder))

83. **stabilityai/stable-diffusion-2-1**  
The 2.1 version of Stable Diffusion, a latent diffusion model with 1B parameters, tuned for improved depth and coherence in generated images at 512×512 resolution, supporting powerful creative art generation under an open license.  
([Hugging Face link](https://huggingface.co/stabilityai/stable-diffusion-2-1))

84. **stabilityai/stable-diffusion-xl-refiner-1.0**  
An SDXL 1.0 refiner model applied at 1024×1024 resolution to enhance and add detail to initial images ([Stability AI releases DeepFloyd IF, a powerful text-to-image model ...](https://stability.ai/news/deepfloyd-if-text-to-image-model#:~:text=Stability%20AI%20releases%20DeepFloyd%20IF%2C,co%2Fspaces%2FDeepFloyd%2FIF)), sharpening textures and refining text details when used in conjunction with the SDXL base model for high-quality outputs.  
([Hugging Face link](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0))

## Image-to-Text
85. **Salesforce/blip2-flan-t5-xxl**  
BLIP-2 is a two-stage vision-language model that connects a ViT-G image encoder to a 11B Flan-T5 XXL text decoder ([InstructBLIP - Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/instructblip#:~:text=In%20this%20paper%2C%20we%20conduct,2%20models)), enabling it to generate rich image captions and answer visual questions with minimal fine-tuning.  
([Hugging Face link](https://huggingface.co/Salesforce/blip2-flan-t5-xxl))

86. **Salesforce/instructblip-flan-t5-xxl**  
InstructBLIP is BLIP-2 fine-tuned on 150K multimodal instructions ([Salesforce/instructblip-flan-t5-xxl - Hugging Face](https://huggingface.co/Salesforce/instructblip-flan-t5-xxl#:~:text=Salesforce%2Finstructblip,Language%20Models)), which endows the model with general-purpose vision-and-language capabilities – from complex image reasoning to following natural language instructions about images.  
([Hugging Face link](https://huggingface.co/Salesforce/instructblip-flan-t5-xxl))

87. **microsoft/git-large-coco**  
GIT (Generative Image Transformer) Large is a 740M-parameter model that was pre-trained on 20 million image-text pairs and fine-tuned on COCO ([3.24 kB - Hugging Face](https://huggingface.co/alexgk/git-large-coco/resolve/dfc0e6f78a682894d06689d3d8c6c374e9826e32/README.md?download=true#:~:text=This%20checkpoint%20is%20%22GIT,See%20table%2011)), enabling it to produce detailed captions or answers for images in a single step.  
([Hugging Face link](https://huggingface.co/microsoft/git-large-coco))

88. **microsoft/OFA-large**  
OFA (One For All) Large is a 340M unified sequence-to-sequence model that handles image captioning, visual QA, and more by encoding images and text together and generating the textual answer, representing a multimodal multitask Transformer approach.  
([Hugging Face link](https://huggingface.co/microsoft/OFA-large))

89. **nlpconnect/vit-gpt2-image-captioning**  
A lightweight image captioning model that uses a ViT visual encoder and a GPT-2 small decoder, fine-tuned together on MS COCO, capable of producing concise descriptive captions for input images (winner of the Kaggle Captioning Challenge).  
([Hugging Face link](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning))

## Robotics-related Models
90. **robotics-diffusion-transformer/rdt-1b**  
RDT-1B is a 1-billion-parameter vision-language-action model that takes in an instruction and RGB images and predicts a sequence of 64 robot actions ([robotics-diffusion-transformer/rdt-1b · Hugging Face](https://huggingface.co/robotics-diffusion-transformer/rdt-1b#:~:text=Image%20RDT,velocity%2C%20and%20even%20wheeled%20locomotion)), using a diffusion policy approach to achieve broad generalization across many manipulation skills.  
([Hugging Face link](https://huggingface.co/robotics-diffusion-transformer/rdt-1b))

91. **openvla/openvla-7b**  
OpenVLA 7B is an open vision-language-action foundation model trained on 970K robot arm trajectories ([openvla/openvla-7b · Hugging Face](https://huggingface.co/openvla/openvla-7b#:~:text=OpenVLA%207B%20%28%60openvla,tuning)): it accepts a camera image and a text command and outputs robot joint controls, supporting a variety of robots and manipulators out of the box.  
([Hugging Face link](https://huggingface.co/openvla/openvla-7b))

92. **lerobot/pi0**  
π0 (PiZero) is a real-world robotics foundation model released by Hugging Face’s LeRobot initiative, which couples vision and language to predict robotic actions via a diffusion policy, aiming for generalist robot control across many tasks and embodiments.  
([Hugging Face link](https://huggingface.co/lerobot/pi0))

93. **VIMA/VIMA**  
VIMA is a transformer-based robot agent (up to 200M parameters) that processes multimodal prompts (e.g. an image showing a goal state and text instructions) and generates a sequence of robotic arm actions ([VIMA/VIMA · Hugging Face](https://huggingface.co/VIMA/VIMA#:~:text=VIMA%20,primarily%20by%20researchers%20at%20Stanford%2FNVIDIA)), enabling few-shot generalization to new manipulation tasks.  
([Hugging Face link](https://huggingface.co/VIMA/VIMA))

## Audio Models
94. **nvidia/diar_sortformer_4spk-v1**  
NVIDIA’s SortFormer is an end-to-end diarization model that addresses the speaker permutation problem by processing speech segments in temporal order ([nvidia/diar_sortformer_4spk-v1 · Hugging Face](https://huggingface.co/nvidia/diar_sortformer_4spk-v1#:~:text=Sortformer%5B1%5D%20is%20a%20novel%20end,end%20diarization%20models)), enabling accurate speaker attribution for up to 4 speakers in a conversation.  
([Hugging Face link](https://huggingface.co/nvidia/diar_sortformer_4spk-v1))

95. **facebook/musicgen-large**  
MusicGen is a text-to-music generation model (approx. 1.5B params) from Meta that produces 12-second musical compositions from text prompts ([facebook/musicgen-large - Hugging Face](https://huggingface.co/facebook/musicgen-large#:~:text=facebook%2Fmusicgen,text%20descriptions%20or%20audio%20prompts)) (optionally conditioning on melody) – generating cohesive songs with rhythm, instrumentation, and harmony.  
([Hugging Face link](https://huggingface.co/facebook/musicgen-large))

96. **facebook/audiogen-medium**  
AudioGen is a transformer-based text-to-audio model by Meta that generates general sound effects from textual descriptions ([facebook/audiogen-medium - Hugging Face](https://huggingface.co/facebook/audiogen-medium#:~:text=AudioGen%20is%20an%20autoregressive%20transformer,AudioGen%20operates%20over%20discrete)) – for example, producing audio of “a dog barking with thunder in the background” with appropriate timing and acoustic properties.  
([Hugging Face link](https://huggingface.co/facebook/audiogen-medium))

97. **Plachta/VALL-E-X**  
An open-source replication of Microsoft’s VALL-E X, a zero-shot TTS model that can clone a speaker’s voice from a short sample and synthesize speech in that voice ([https://raw.githubusercontent.com/Plachtaa/VALL-E-...](https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/README-ZH.md#:~:text=...%20raw.githubusercontent.com%20%20VALL,X%29%20%5B%21%5BOpen%20In%20Colab%5D%28https%3A%2F%2Fcolab.research.google.com)) – effectively achieving high-quality voice conversion and cross-lingual speaking styles.  
([Hugging Face link](https://huggingface.co/Plachta/VALL-E-X))

98. **speechbrain/sepformer-wsj02mix**  
A SpeechBrain model that performs source separation on mixed speech, using a dual-path Transformer (SepFormer) to separate two speakers’ voices from a single recording with state-of-the-art accuracy (trained on WSJ0-2mix corpus).  
([Hugging Face link](https://huggingface.co/speechbrain/sepformer-wsj02mix))

99. **facebook/encodec_24khz**  
EnCodec is a neural audio codec by Meta that encodes audio waveforms into a compressed latent representation and decodes them back with high fidelity ([facebookresearch/encodec: State-of-the-art deep learning ... - GitHub](https://github.com/facebookresearch/encodec#:~:text=GitHub%20github.com%20%20from_pretrained%28,to%20the%20correct%20sampling)) – it can compress 24 kHz audio to a low bitrate while preserving quality, enabling downstream generative models to work in the discrete audio domain.  
([Hugging Face link](https://huggingface.co/facebook/encodec_24khz))

100. **speechbrain/spkrec-ecapa-voxceleb**  
A pretrained ECAPA-TDNN model for speaker recognition by SpeechBrain that produces 192-dimensional embeddings for voices ([speechbrain/spkrec-ecapa-voxceleb - Hugging Face](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb#:~:text=This%20repository%20provides%20all%20the,can%20be%20used%20to)) – given an audio sample, it creates a “voice fingerprint,” enabling highly accurate speaker verification (EER <1% on VoxCeleb).  
([Hugging Face link](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb))